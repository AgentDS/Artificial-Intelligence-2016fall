function [best_nn_params, J_history] = nngradientDescent(input_layer_size, ...
                                           hidden_layer_size, ...
                                           num_labels,...
                                           initial_nn_params, ...
                                           alpha, num_iters,...
                                           X, y, lambda)
%GRADIENTDESCENTMULTI Performs gradient descent to learn theta
%   theta = GRADIENTDESCENTMULTI(x, y, theta, alpha, num_iters) updates theta by
%   taking num_iters gradient steps with learning rate alpha

% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);
Theta1 = reshape(initial_nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
                         hidden_layer_size, (input_layer_size + 1));

Theta2 = reshape(initial_nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
                         num_labels, (hidden_layer_size + 1));
nn_params= initial_nn_params;                     
for iter = 1:num_iters
    [J grad] = nnCostFunction(nn_params, ...
                              input_layer_size, ...
                              hidden_layer_size, ...
                              num_labels, ...
                              X, y, lambda);
    Theta1_grad = reshape(grad(1:hidden_layer_size * (input_layer_size + 1)), ...
                         hidden_layer_size, (input_layer_size + 1));

    Theta2_grad = reshape(grad((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
                         num_labels, (hidden_layer_size + 1));
    Theta1= Theta1 - alpha*Theta1_grad;
    Theta2= Theta2 - alpha*Theta2_grad;
    J_history(iter) = J;
    nn_params= [Theta1(:) ; Theta2(:)];
end
best_nn_params= nn_params;
end
